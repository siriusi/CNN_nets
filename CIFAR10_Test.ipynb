{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWUdwHrBkctr"
   },
   "source": [
    "# CIFAR10_Test\n",
    "\n",
    "使用CIFAR10数据集对CNN进行训练及测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Ykfy4xGQkct0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z_tomcato/assignment1/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "from mobile_net import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 125,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 31366,
     "status": "ok",
     "timestamp": 1522747101694,
     "user": {
      "displayName": "Y Zzhuangy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115487754759868122157"
     },
     "user_tz": -480
    },
    "id": "zG95i-smkcuA",
    "outputId": "e588e385-dee3-40c0-cb6f-e792f146bc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if sys.platform == \"linux\" :\n",
    "    cifar10_dir = \"../assignment2/assignment2/cs231n/datasets/cifar-10-batches-py\"\n",
    "else:\n",
    "    cifar10_dir = 'cs231n/datasets'\n",
    "\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    #cifar10_dir = 'cs231n/datasets'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "flags = tf.flags\n",
    "\"\"\"\n",
    "flags.DEFINE_integer('batch_size', 100, 'batch size')\n",
    "flags.DEFINE_integer('num_epochs', 35, 'number of epochs')\n",
    "flags.DEFINE_float('learning_rate', 0.04, 'init learning rate')\n",
    "flags.DEFINE_float('dropout', 0.5, 'define dropout keep probability')\n",
    "flags.DEFINE_float('max_grad_norm', 5.0, 'define maximum gradient normalize value')\n",
    "flags.DEFINE_float('normalize_decay', 5.0, 'batch normalize decay rate')\n",
    "flags.DEFINE_float('weight_decay', 0.0002, 'L2 regularizer weight decay rate')\n",
    "\n",
    "flags.DEFINE_integer('print_every', 5, 'how often to print training status')\n",
    "flags.DEFINE_string('name', None, 'name of result save dir')\n",
    "\"\"\"\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "def run_model(session, Xd, yd, Xv, yv, epochs=3, batch_size=100,print_every=10, learning_rate = 0.04, dropout = 0.5):\n",
    "    print(\"Batch dataset initialized.\\n# of training data: {}\\n# of test data: {}\\n# of class: {}\"\n",
    "          .format(Xd.shape[0], Xv.shape[0], 10))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "    \n",
    "    cnn_net = MobileNet(Xd.shape, 10)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # tensorboard setting\n",
    "        train_summary = tf.summary.merge([tf.summary.scalar(\"train_loss\", cnn_net.loss),\n",
    "                                          tf.summary.scalar(\"train_accuracy\", cnn_net.accuracy)])\n",
    "        test_summary = tf.summary.merge([tf.summary.scalar(\"val_loss\", cnn_net.loss),\n",
    "                                         tf.summary.scalar(\"val_accuracy\", cnn_net.accuracy)])\n",
    "        \n",
    "        fileName = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "        fileName = os.path.normcase(\"./result/\" + fileName)\n",
    "        print(fileName)\n",
    "        summary_writer = tf.summary.FileWriter(fileName, sess.graph)\n",
    "        yd = yd.reshape([Xd.shape[0], 1])\n",
    "        yv = yv.reshape([Xv.shape[0], 1])\n",
    "        for current_epoch in range(epochs):\n",
    "            # training step\n",
    "            ###for x_batch, y_batch in batch_set.batches():\n",
    "            print(\"#############################Epoch Start##############################\")\n",
    "            \n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                start = time.time()\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = np.int32(train_indicies[start_idx:start_idx+batch_size])\n",
    "                feed = {cnn_net.train_data:  Xd[idx,:, :, :], cnn_net.targets: yd[idx, :],\n",
    "                        cnn_net.learning_rate: learning_rate, cnn_net.dropout: dropout, \n",
    "                        cnn_net.is_training : True}\n",
    "                _, global_step, loss, accuracy, summary = \\\n",
    "                    sess.run([cnn_net.train_op, cnn_net.global_step, cnn_net.loss,\n",
    "                              cnn_net.accuracy, train_summary], feed_dict=feed)\n",
    "                summary_writer.add_summary(summary, global_step)\n",
    "                if global_step % print_every == 0:\n",
    "                    print(\"{}/{} ({} epochs) step, loss : {:.6f}, accuracy : {:.3f}, time/batch : {:.3f}sec\"\n",
    "                          .format(global_step, int(round(Xd.shape[0]/batch_size)) * epochs, current_epoch,\n",
    "                                  loss, accuracy, time.time() - start))\n",
    "            #验证集\n",
    "            start, avg_loss, avg_accuracy = time.time(), 0, 0\n",
    "\n",
    "            feed = {cnn_net.train_data: Xv,cnn_net.targets: yv,\n",
    "                    cnn_net.learning_rate: learning_rate, cnn_net.dropout: 1.0, cnn_net.is_training : True}\n",
    "            loss, accuracy, summary = sess.run([cnn_net.loss, cnn_net.accuracy, test_summary], feed_dict=feed)\n",
    "            avg_loss = loss\n",
    "            avg_accuracy = accuracy \n",
    "            summary_writer.add_summary(summary, current_epoch)\n",
    "            print(\"{} epochs test result. loss : {:.6f}, accuracy : {:.3f}, time/batch : {:.3f}sec\"\n",
    "                  .format(current_epoch, avg_loss , avg_accuracy , time.time() - start))\n",
    "            print(\"\\n\")\n",
    "    return avg_loss,avg_accuracy      \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dataset initialized.\n",
      "# of training data: 500\n",
      "# of test data: 1000\n",
      "# of class: 10\n",
      "./result/20180427_013049\n",
      "#############################Epoch Start##############################\n",
      "0 epochs test result. loss : 2.032577, accuracy : 0.247, time/batch : 18.479sec\n",
      "\n",
      "\n",
      "#############################Epoch Start##############################\n",
      "10/30 (1 epochs) step, loss : 1.370695, accuracy : 0.670, time/batch : 8.631sec\n",
      "1 epochs test result. loss : 1.894448, accuracy : 0.309, time/batch : 18.368sec\n",
      "\n",
      "\n",
      "#############################Epoch Start##############################\n",
      "2 epochs test result. loss : 1.793587, accuracy : 0.352, time/batch : 18.800sec\n",
      "\n",
      "\n",
      "#############################Epoch Start##############################\n",
      "20/30 (3 epochs) step, loss : 0.285068, accuracy : 0.990, time/batch : 8.906sec\n",
      "3 epochs test result. loss : 1.805395, accuracy : 0.343, time/batch : 18.971sec\n",
      "\n",
      "\n",
      "#############################Epoch Start##############################\n",
      "4 epochs test result. loss : 1.875831, accuracy : 0.337, time/batch : 18.775sec\n",
      "\n",
      "\n",
      "#############################Epoch Start##############################\n",
      "30/30 (5 epochs) step, loss : 0.011491, accuracy : 1.000, time/batch : 8.602sec\n",
      "5 epochs test result. loss : 2.127899, accuracy : 0.332, time/batch : 18.471sec\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #print('Training')\n",
    "    run_model(sess,X_train[:500],y_train[:500],X_val,y_val, epochs=6, batch_size=100,print_every=10, learning_rate = 0.001)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "TensorFlow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
