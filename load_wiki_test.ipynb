{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_margin(img, face_loc):\n",
    "    crop_h = int(0.4 * (face_loc[3] - face_loc[1]))\n",
    "    crop_w = int(0.4 * (face_loc[2] - face_loc[0]))\n",
    "    img_h = img.shape[0]\n",
    "    img_w = img.shape[1]\n",
    "    #new_crop_low = crop_h + face_loc[1] - crop_h\n",
    "    new_crop_high = crop_h + face_loc[3] + crop_h\n",
    "    #new_crop_left = crop_w + face_loc[0] - crop_w\n",
    "    new_crop_right = crop_w + face_loc[2] + crop_w\n",
    "    replicate = cv2.copyMakeBorder(img, crop_h, crop_h, crop_w, crop_w, cv2.BORDER_REPLICATE)\n",
    "    crop_face_img = replicate[face_loc[1] : new_crop_high, face_loc[0] : new_crop_right]\n",
    "    return crop_face_img\n",
    "\n",
    "#产生新图像\n",
    "def add_augment_img(img):\n",
    "    translateX = np.random.uniform(-0.1, 0.1)\n",
    "    translateY = np.random.uniform(-0.1, 0.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    rotate_angle = np.random.uniform(-10, 10)\n",
    "    \n",
    "    def translate(image, x, y):\n",
    "        # 定义平移矩阵\n",
    "        M = np.float32([[1, 0, x], [0, 1, y]])\n",
    "        shifted = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        # 返回转换后的图像\n",
    "        return shifted\n",
    "    \n",
    "    # 定义旋转rotate函数\n",
    "    def rotate(image, angle, center=None, scale=1.0):\n",
    "        # 获取图像尺寸\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        # 若未指定旋转中心，则将图像中心设为旋转中心\n",
    "        if center is None:\n",
    "            center = (w / 2, h / 2)\n",
    "\n",
    "        # 执行旋转\n",
    "        M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "        rotated = cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "        # 返回旋转后的图像\n",
    "        return rotated\n",
    "    \n",
    "    img = translate(img, translateX, translateY)\n",
    "    cv2.imshow(\"translate\", img)\n",
    "    \n",
    "    rotated = rotate(img, rotate_angle, scale = scale)\n",
    "    \n",
    "    print(translateX, translateY, scale, rotate_angle)\n",
    "    cv2.imshow(\"getRotationMatrix2D\", rotated)\n",
    "    return rotated\n",
    "\n",
    "    \n",
    "def load_wiki(wiki_path, num_data = None, target_size = (224,224)):\n",
    "    mat_path = wiki_path + 'wiki_with_age.mat'\n",
    "    \n",
    "    data = sio.loadmat(mat_path)\n",
    "    wiki_data = data['wiki'][0][0]\n",
    "   \n",
    "    if num_data is None:\n",
    "        num_data = len(wiki_data[6][0])\n",
    "        \n",
    "    \n",
    "    X_data_age = np.zeros([num_data, 224, 224, 3],dtype = \"uint8\")\n",
    "    y_data_age = np.zeros([num_data],dtype = \"uint8\")\n",
    "    \n",
    "    X_data_gender = np.zeros([num_data, 224, 224, 3],dtype = \"uint8\")\n",
    "    y_data_gender = np.zeros([num_data],dtype = \"uint8\")\n",
    "    \n",
    "    data_count = 0\n",
    "    #len(wiki_data[6][0])\n",
    "    num_every_age = int(num_data / 100)\n",
    "    counter_every_age = np.zeros(100)\n",
    "    index_every_age = np.zeros(100, num_every_age)\n",
    "    \n",
    "    num_every_gender = int(num_data / 2)\n",
    "    counter_every_gender = np.zeros(2)\n",
    "    index_every_gender = np.zeros(2, num_every_gender)\n",
    "    \n",
    "    for i in range(num_data):\n",
    "\n",
    "        face_score =wiki_data[6][0][i]\n",
    "        if face_score != float(\"-inf\"):         #如果face_score == -inf说明不存在脸,这个比例比较大，因此先排除\n",
    "            full_path = wiki_path + wiki_data[2][0][i][0]\n",
    "            img = cv2.imread(full_path)\n",
    "            age = wiki_data[8][0][i]            #有些age在正常值之外要排除\n",
    "            date_of_birth = wiki_data[0][0][i]  #下面的657438是出生于1800年的Matlab serial date number\n",
    "            gender = wiki_data[3][0][i]         #有一些gender==None会引起一场，需要排除\n",
    "            if img is not None and gender == gender and date_of_birth > 657438 and age >= 0 and age <= 100:\n",
    "                face_loc = wiki_data[5][0][i][0]\n",
    "                face_loc = face_loc.astype(\"int32\")\n",
    "                roi_img = add_margin(img, face_loc)    \n",
    "                face_img = cv2.resize(roi_img, target_size)\n",
    "                \n",
    "                index_every_age[age, counter_every_age[age]] = data_count\n",
    "                counter_every_age[age] += 1\n",
    "                \n",
    "                index_every_gender[gender, counter_every_gender[gender]] = data_count\n",
    "                counter_every_gender[gender] += 1\n",
    "                \n",
    "                full_X_data[data_count] = face_img                \n",
    "               \n",
    "                data_count += 1\n",
    "                \n",
    "    for cur_age in range(100):            \n",
    "        X_data_age[cur_age : cur_age * num_every_age] = full_X_data[index_every_age[cur_age, 0 : num_every_age]]\n",
    "        if counter_every_age[cur_age] < num_every_age:\n",
    "            for cur_augment in range(num_every_age - counter_every_age[cur_age]):\n",
    "                cur_choice = np.random.choice(counter_every_age[cur_age], replace = True)\n",
    "                np.append(X_data_age, add_augment_img(full_X_data[cur_choice]))\n",
    "        y_data_age[cur_age : cur_age * num_every_age] = cur_age\n",
    "        \n",
    "    for cur_gender in range(2):            \n",
    "        X_data_gender[cur_gender : cur_gender * num_every_gender] = \\\n",
    "                                full_X_data[index_every_gender[cur_gender, 0 : num_every_gender]]\n",
    "        if counter_every_gender[cur_gender] < num_every_gender:\n",
    "            for cur_augment in range(num_every_gender - counter_every_gender[cur_gender]):\n",
    "                cur_choice = np.random.choice(counter_every_gender[cur_gender], replace = True)\n",
    "                np.append(X_data_gender, add_augment_img(full_X_data[cur_choice]))\n",
    "        y_data_gender[cur_gender : cur_gender * num_every_gender] = cur_gender\n",
    "    \n",
    "    \n",
    "    return X_data_age, y_data_age, X_data_gender, y_data_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_data(X_data, y_data, num_training=49000, num_validation=1000, num_test=1000, subtract_mean = True):\n",
    "   \n",
    "    X_train = X_data[:-(num_validation + num_test)]\n",
    "    y_train = y_data[:-(num_validation + num_test)]\n",
    "    X_val = X_data[X_train.shape[0] : -num_test]\n",
    "    y_val = y_data[X_train.shape[0] : -num_test]\n",
    "    X_test = X_data[-num_test:]\n",
    "    y_test = y_data[-num_test:]\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_data, axis=0).astype(\"uint8\")\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "      'X_train': X_train, 'y_train': y_train,\n",
    "      'X_val': X_val, 'y_val': y_val,\n",
    "      'X_test': X_test, 'y_test': y_test,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09507326165652905 0.053617105734280524 0.9176089286351296 -4.9122970267128\n",
      "-0.025307055127768943 0.07990387675439001 0.9685518348045866 4.220507087966306\n",
      "-0.06175113416187144 -0.006301875829342357 0.9893250103070833 4.072922702103096\n",
      "0.0040480108870521725 -0.02681021195504596 1.0037402448139587 -0.4593939171329424\n",
      "-0.04062166173265811 -0.0008792262335024503 0.9631703099811444 3.8945957781463942\n",
      "0.09209542830796588 0.08714034991786054 1.069964880925668 0.4513897763161019\n",
      "-0.025393573052195056 -0.05434495519057945 1.0458827820972143 -3.6315535684953515\n"
     ]
    }
   ],
   "source": [
    "#不带h5py的\n",
    "wiki_path = \"\"\n",
    "if sys.platform == \"linux\" :\n",
    "    wiki_path = \"/devdata/wiki/\"\n",
    "else:\n",
    "    wiki_path = \"G:\\\\MachineLearning\\\\wiki\\\\wiki\\\\\"\n",
    "    #wiki_path = \"D:\\\\Z\\\\wiki\\\\\"\n",
    "\n",
    "mat_path = wiki_path + 'wiki_with_age.mat'\n",
    "\n",
    "X_data, y_data  = load_wiki(wiki_path, 8)\n",
    "    \n",
    "wiki_cropface_dataset = get_wiki_data(X_data, y_data, num_training=49000, num_validation=10, num_test=0)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b547079a1c09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwiki_cropface_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/devdata/wiki_cropface_data.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mload_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwiki_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mwiki_cropface_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wiki_cropface_group\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "wiki_path = \"\"\n",
    "if sys.platform == \"linux\" :\n",
    "    wiki_path = \"/devdata/wiki/\"\n",
    "else:\n",
    "    wiki_path = \"G:\\\\MachineLearning\\\\wiki\\\\wiki\\\\\"\n",
    "\n",
    "mat_path = wiki_path + 'wiki_with_age.mat'\n",
    "\n",
    "# Create a new file\n",
    "f = None\n",
    "X_data = None\n",
    "y_data = None\n",
    "#wiki_crop_dataset = f.create_dataset('wiki_cropface_data', dtype = \"int32\")\n",
    "if os.path.exists('/devdata/wiki_cropface_data.h5'):\n",
    "    f = h5py.File('/devdata/wiki_cropface_data.h5', \"r\")\n",
    "    wiki_cropface_group = f[\"wiki_cropface_group\"]\n",
    "    X_data = np.array(wiki_cropface_group[\"X_data\"][:])\n",
    "    y_data = np.array(wiki_cropface_group[\"y_data\"][:])\n",
    "else:\n",
    "    f = h5py.File('/devdata/wiki_cropface_data.h5',\"w\")\n",
    "    X_data, y_data  = load_wiki(wiki_path, 1)\n",
    "    wiki_cropface_group = f.create_group(\"wiki_cropface_group\")\n",
    "    wiki_cropface_group.create_dataset('X_data', dtype = \"uint8\", data = X_data)\n",
    "    wiki_cropface_group.create_dataset('y_data', dtype = \"uint8\", data = y_data)\n",
    "    \n",
    "print(X_data.shape)\n",
    "wiki_cropface_dataset = get_wiki_data(X_data, y_data, num_training=49000, num_validation=10, num_test=0)\n",
    "f.close()\n",
    "\"\"\"\n",
    "test_img = X_data[0,:,:,:]\n",
    "cv2.imshow(\"test_img\", test_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
